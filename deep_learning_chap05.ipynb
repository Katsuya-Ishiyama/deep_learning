{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ゼロから作るDeep Learning\n",
    "## 5章 誤差逆伝播法\n",
    "### 5.4 単純なレイヤの実装\n",
    "#### 5.4.1 乗法レイヤの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 乗法レイヤのクラスを実装する\n",
    "class MulLayer:\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "\n",
    "    def forward(self, x, y):\n",
    "\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        out = x * y\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "\n",
    "        dx = dout * self.y\n",
    "        dy = dout * self.x\n",
    "\n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MulLayerクラスを使うと、図5-16は順伝播は次のように実装できる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "price = 220.00000000000003\n"
     ]
    }
   ],
   "source": [
    "# 実装例\n",
    "apple = 100\n",
    "apple_num = 2\n",
    "tax = 1.1\n",
    "\n",
    "# layer\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "\n",
    "# forward\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num)\n",
    "price = mul_tax_layer.forward(apple_price, tax)\n",
    "\n",
    "# 結果を表示する\n",
    "print('price =', price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "各変数に関する微分は、backwardメソッドを使って次のように実装できる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(dapple, dapple_num, dtax) = (2.2, 110.00000000000001, 200)\n"
     ]
    }
   ],
   "source": [
    "dprice = 1\n",
    "\n",
    "dapple_price, dtax = mul_tax_layer.backward(dprice)\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n",
    "\n",
    "# 結果を表示する\n",
    "print('(dapple, dapple_num, dtax) =', (dapple, dapple_num, dtax))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4.2 加算レイヤの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AddLayer(object):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        # 自分で追加した部分\n",
    "        # あとでx, yに何が入力されているのかを確認できるようにする\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "\n",
    "    def forward(self, x, y):\n",
    "\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        out = x + y\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "\n",
    "        dx = dout * 1\n",
    "        dy = dout * 1\n",
    "\n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MulLayerとAddLayerを使って図5-17を解いてみる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "price = 715.0000000000001\n",
      "(dapple_num, dapple, dorange_num, dorange, dtax) = (110.00000000000001, 2.2, 165.0, 3.3000000000000003, 650)\n"
     ]
    }
   ],
   "source": [
    "apple = 100\n",
    "apple_num = 2\n",
    "orange = 150\n",
    "orange_num = 3\n",
    "tax = 1.1\n",
    "\n",
    "# layer\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_orange_layer = MulLayer()\n",
    "add_apple_orange_layer = AddLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "\n",
    "# forward\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num)\n",
    "orange_price = mul_orange_layer.forward(orange, orange_num)\n",
    "all_price = add_apple_orange_layer.forward(apple_price, orange_price)\n",
    "price = mul_tax_layer.forward(all_price, tax)\n",
    "\n",
    "# backward\n",
    "dprice = 1\n",
    "\n",
    "dall_price, dtax = mul_tax_layer.backward(dprice)\n",
    "dapple_price, dorange_price = add_apple_orange_layer.backward(dall_price)\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n",
    "dorange, dorange_num = mul_orange_layer.backward(dorange_price)\n",
    "\n",
    "# 結果を表示\n",
    "print('price =', price)\n",
    "print('(dapple_num, dapple, dorange_num, dorange, dtax) =', (dapple_num, dapple, dorange_num, dorange, dtax))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 活性化関数レイヤの実装\n",
    "#### 5.5.1 ReLUレイヤ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReLU(Rectified Linear Unit)は\n",
    "\\begin{equation}\n",
    "y = \\begin{cases}\n",
    "    x & (x > 0) \\\\\n",
    "    0 & (x \\le 0)\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "この微分は\n",
    "\\begin{equation}\n",
    "\\frac{\\partial y}{\\partial x} = \\begin{cases}\n",
    "    1 & (x > 0) \\\\\n",
    "    0 & (x \\le 0)\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "となる。  \n",
    "（本来は$x = 0$では微分は定義できないはずなので、便宜上$\\frac{\\partial y}{\\partial x} = 0$と定義しているのか？）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Relu(object):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.mask = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        self.is_negative = (x <= 0)\n",
    "\n",
    "        out = x.copy()\n",
    "        out[is_negative] = 0\n",
    "        \n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "\n",
    "        dx = dout.copy()\n",
    "        dx[self.is_negative] = 0\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.5.2 Sigmoidレイヤ\n",
    "シグモイド関数は\n",
    "\\begin{equation}\n",
    "y = \\frac{1}{1 + \\exp(-x)}.\n",
    "\\end{equation}\n",
    "図5-21のSigmoidレイヤの逆伝播計算は\n",
    "\\begin{equation}\n",
    "\\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial y}y(1 - y)\n",
    "\\end{equation}\n",
    "と表せるので、上式を利用してSigmoidレイヤを実装する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Sigmoid(object):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        self.out = 1 / (1 / np.exp(-x))\n",
    "\n",
    "        return self.out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "\n",
    "        dx = dout * self.out * (1.0 - self.out)\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 5.6 Affine/Softmaxレイヤの実装\n",
    "#### 5.6.2 バッチ版Affineレイヤ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Affine(object):\n",
    "\n",
    "    def __init__(self, W, b):\n",
    "\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.x = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        self.x = x\n",
    "        out = np.dot(x, self.W) + self.b\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "\n",
    "        return dx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.6.3 Softmax-with-Lossレイヤ\n",
    "Softmaxレイヤを損失関数である交差エントロピー誤差(Cross Entropy Error)を含めて、Softmax-with-Lossレイヤという名前のレイヤで実装する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SoftmaxWithLoss(object):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.loss = None\n",
    "        self.y = None\n",
    "        self.t = None\n",
    "\n",
    "    def forward(self, x, t):\n",
    "\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "\n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "\n",
    "        batch_size = self.t.shape[0]\n",
    "        dx = (self.y - self.t) / batch_size\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 5.7 誤差逆伝播法の実装\n",
    "#### 5.7.2 誤差逆伝播法に対応したニューラルネットワークの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from common.layers import *\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "class TwoLayerNet(object):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "\n",
    "        # 重みの初期化\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "        # レイヤの生成\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
    "\n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self, x):\n",
    "\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t):\n",
    "\n",
    "        y = self.predict(x)\n",
    "\n",
    "        return self.lastLayer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t):\n",
    "\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "\n",
    "        if t.ndim != 1:\n",
    "            t = np.argmax(t, axis=1)\n",
    "\n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "\n",
    "        return accuracy\n",
    "\n",
    "    def numerical_gradient(self, x, t):\n",
    "\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "\n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # Settings\n",
    "        grads = {}\n",
    "        grads['W1'] = self.layers['Affine1'].dW\n",
    "        grads['b1'] = self.layers['Affine1'].db\n",
    "        grads['W2'] = self.layers['Affine2'].dW\n",
    "        grads['b2'] = self.layers['Affine2'].db\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.7.3 誤差逆伝播法の勾配確認\n",
    "数値微分で勾配を求めた結果と、誤差逆電波法で求めた勾配の結果が一致することを確認する作業を勾配確認(gradient check)という。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1:2.2394887826816718e-13\n",
      "W2:7.635732876404831e-13\n",
      "b1:7.097339849073542e-13\n",
      "b2:1.1990407833284423e-10\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from dataset.mnist import load_mnist\n",
    "from common.two_layer_net import TwoLayerNet\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "x_batch = x_train[:3]\n",
    "t_batch = t_train[:3]\n",
    "\n",
    "grad_numerical = network.numerical_gradient(x_batch, t_batch)\n",
    "grad_backprop = network.gradient(x_batch, t_batch)\n",
    "\n",
    "# 各重みの絶対誤差の平均を求める\n",
    "for key in grad_numerical.keys():\n",
    "    diff = np.average(np.abs(grad_backprop[key] - grad_numerical[key]))\n",
    "    print('{0}:{1}'.format(key, diff))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.7.4 誤差逆伝播法を使った学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09437 0.08910\n",
      "0.90650 0.90890\n",
      "0.92793 0.93090\n",
      "0.93757 0.93630\n",
      "0.94653 0.94500\n",
      "0.95187 0.95140\n",
      "0.95663 0.95290\n",
      "0.96027 0.95800\n",
      "0.96385 0.95870\n",
      "0.96708 0.96360\n",
      "0.96935 0.96450\n",
      "0.97218 0.96700\n",
      "0.97215 0.96830\n",
      "0.97340 0.96770\n",
      "0.97550 0.96800\n",
      "0.97605 0.96810\n",
      "0.97847 0.96990\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from dataset.mnist import load_mnist\n",
    "from common.two_layer_net import TwoLayerNet\n",
    "\n",
    "# データの読み込み\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "\n",
    "    # 誤差逆伝播法によって勾配を求める\n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "\n",
    "    # 更新\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "\n",
    "        print('{0:.5f} {1:.5f}'.format(train_acc, test_acc))\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
