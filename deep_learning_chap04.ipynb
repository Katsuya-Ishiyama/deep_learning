{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ゼロから作るDeep Learning\n",
    "# 4章 ニューラルネットワークの学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 データから学習する\n",
    "#### 4.1.1 データ駆動"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 損失関数\n",
    "#### 4.2.1 2乗和誤差\n",
    "損失関数の中で最も有名なものは次の2乗和誤差(Mean Squared Error)である。\n",
    "（平均2乗誤差の方が一般的なイメージか？）\n",
    "\\begin{equation}\n",
    "E = \\frac{1}{2} \\sum_{k} (y_{k} - t_{k})^{2}\n",
    "\\end{equation}\n",
    "ここで、$y_{k}$はニューラルネットワークの出力、$t_{k}$は教師データを表す。  \n",
    "$y_{k}$の分布に正規分布を仮定した場合の対数尤度関数の定数項を除いたものである。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 必要なモジュールを読み込む\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 平均二乗誤差の実装\n",
    "def mean_squared_error(y, t):\n",
    "    \n",
    "    return 0.5 * np.sum((y - t)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Mean Squared Error -----\n",
      "Example 1: 0.0975\n",
      "Example 2: 0.5975\n"
     ]
    }
   ],
   "source": [
    "# 実際に試してみる\n",
    "print('----- Mean Squared Error -----')\n",
    "t = np.array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0])\n",
    "\n",
    "# 2の確率が最も悪い場合\n",
    "y1 = np.array([0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0])\n",
    "print('Example 1:', mean_squared_error(y1, t))\n",
    "\n",
    "# 2の確率が最も高い場合\n",
    "y2 = np.array([0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0])\n",
    "print('Example 2:', mean_squared_error(y2, t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.2 交差エントロピー誤差\n",
    "交差エントロピー誤差(Cross Entropy Error)もよく用いられる損失関数で、次式で表される。\n",
    "\\begin{equation}\n",
    "E = -\\sum_{k} t_{k} \\log y_{k}\n",
    "\\end{equation}\n",
    "これは多項ロジスティック回帰で回帰係数を求める際の対数尤度関数である。\n",
    "(Bishop, pp.235を参照)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 交差エントロピー誤差の実装\n",
    "def cross_entropy_error(y, t):\n",
    "    # yが0だった場合でも計算を進められるようにするため\n",
    "    delta = 1e-7\n",
    "\n",
    "    return -np.sum(t * np.log(y + delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Cross Entropy Error -----\n",
      "Example 1: 0.510825457099\n",
      "Example 2: 2.30258409299\n"
     ]
    }
   ],
   "source": [
    "# 実際に試してみる\n",
    "print('----- Cross Entropy Error -----')\n",
    "t = np.array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0])\n",
    "\n",
    "y1 = np.array([0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0])\n",
    "print('Example 1:', cross_entropy_error(y1, t))\n",
    "\n",
    "y2 = np.array([0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0])\n",
    "print('Example 2:', cross_entropy_error(y2, t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.3 ミニバッチ学習\n",
    "先程の交差エントロピー誤差はデータが1つの場合のものである。\n",
    "これを$N$個のデータに適用できるように拡張すると次のようになる。\n",
    "\\begin{equation}\n",
    "E = - \\frac{1}{N} \\sum_{n} \\sum_{k} t_{nk} \\log y_{nk}\n",
    "\\end{equation}\n",
    "\n",
    "ミニバッチ学習とは数百万、数千万という膨大なデータの中から一部を抜き出し、その抜き出したデータを使って学習を行うこと。\n",
    "ミニバッチ学習のために訓練データの中から指定された個数のデータをランダムに選び出すコードを書く。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000, 10)\n"
     ]
    }
   ],
   "source": [
    "# MNISTデータを読み込む\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(t_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 実際に訓練データを抜き出すコード\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 10\n",
    "batch_mask = np.random.choice(train_size, batch_size)\n",
    "x_batch = x_train[batch_mask]\n",
    "t_batch = t_train[batch_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.4 ［バッチ対応版］交差エントロピー誤差の実装\n",
    "データが1つの場合と、データがバッチとしてまとめられて入力される場合の両方のケースに対応できるようにする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = t.reshape(1, y.size)\n",
    "    \n",
    "    batch_size = y.shape[0]\n",
    "    \n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t])) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test = [ 0.1  2.   0.5  9. ]\n",
      "test.reshape(1, test.size) = [[ 0.1  2.   0.5  9. ]]\n"
     ]
    }
   ],
   "source": [
    "# reshapeメソッドを知らなかったので、動作検証してみる\n",
    "test = np.array([0.1, 2, 0.5, 9])\n",
    "print('test =', test)\n",
    "print('test.reshape(1, test.size) =', test.reshape(1, test.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = [[  0.5   1.    0.1]\n",
      " [  0.2   2.    7. ]\n",
      " [ 10.    3.    0. ]]\n",
      "t = [2 0 1]\n",
      "y[np.arange(3), t] = [ 0.1  0.2  3. ]\n",
      "y[:, t] = [[  0.1   0.5   1. ]\n",
      " [  7.    0.2   2. ]\n",
      " [  0.   10.    3. ]]\n"
     ]
    }
   ],
   "source": [
    "# 複雑な部分の動作を確認する\n",
    "y = np.array([[0.5, 1, 0.1],\n",
    "              [0.2, 2, 7],\n",
    "              [10, 3, 0]])\n",
    "print('y =', y)\n",
    "\n",
    "t = np.array([2, 0, 1])\n",
    "print('t =', t)\n",
    "\n",
    "print('y[np.arange(3), t] =', y[np.arange(3), t])\n",
    "print('y[:, t] =', y[:, t])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 4.3 数値微分\n",
    "#### 4.3.1 微分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def numerical_diff(f, x):\n",
    "\n",
    "    h = 1e-4\n",
    "\n",
    "    return (f(x + h) - f(x - h)) / (2 * h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.2 数値微分の例\n",
    "上記の数値微分で次の関数を試してみる。\n",
    "\\begin{equation}\n",
    "y = 0.01 x^{2} + 0.1 x\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical_diff(f1,  5) = 0.1999999999990898\n",
      "numerical_diff(f1, 10) = 0.2999999999986347\n"
     ]
    }
   ],
   "source": [
    "def f1(x):\n",
    "\n",
    "    return 0.01 * x**2 + 0.1 * x\n",
    "\n",
    "# 数値微分を試す\n",
    "print('numerical_diff(f1,  5) =', numerical_diff(f1, 5))\n",
    "print('numerical_diff(f1, 10) =', numerical_diff(f1, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.3 偏微分\n",
    "2変数関数\n",
    "\\begin{equation}\n",
    "f(x_{0}, x_{1}) = x_{0}^{2} + x_{1}^{2}\n",
    "\\end{equation}\n",
    "について、  \n",
    "(1)$x_{0}=3$, $x_{1}=4$のときの$x_{0}$に対する偏微分$\\frac{\\partial f}{\\partial x_{0}}$を求めよ。  \n",
    "(2)$x_{0}=3$, $x_{1}=4$のときの$x_{1}$に対する偏微分$\\frac{\\partial f}{\\partial x_{1}}$を求めよ。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical_diff(f2_tmp1, 3.0) = 6.00000000000378\n",
      "numerical_diff(f2_tmp2, 4.0) = 7.999999999999119\n"
     ]
    }
   ],
   "source": [
    "def f2(x):\n",
    "    \n",
    "    return np.sum(x**2)\n",
    "\n",
    "\n",
    "def f2_tmp1(x0):\n",
    "\n",
    "    return x0 ** 2 + 4.0 ** 2\n",
    "\n",
    "\n",
    "def f2_tmp2(x1):\n",
    "    \n",
    "    return 3.0 ** 2 + x1 ** 2\n",
    "\n",
    "print('numerical_diff(f2_tmp1, 3.0) =', numerical_diff(f2_tmp1, 3.0))\n",
    "print('numerical_diff(f2_tmp2, 4.0) =', numerical_diff(f2_tmp2, 4.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 勾配\n",
    "2変数関数の勾配(gradient)\n",
    "\\begin{equation}\n",
    "\\nabla f(x0, x1) = (\\frac{\\partial f}{\\partial x_{0}}, \\frac{\\partial f}{\\partial x_{1}})\n",
    "\\end{equation}\n",
    "を実装する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical_gradient(f2, np.array([3, 4])) = [ 6.  8.]\n",
      "numerical_gradient(f2, np.array([0, 2])) = [ 0.  4.]\n",
      "numerical_gradient(f2, np.array([3, 0])) = [ 6.  0.]\n"
     ]
    }
   ],
   "source": [
    "# 全微分の実装\n",
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4\n",
    "    grad = np.zeros_like(x)\n",
    "\n",
    "    for i, tmp in enumerate(x):\n",
    "        tmp_x = x.copy()\n",
    "        \n",
    "        # f(x + h)の計算\n",
    "        tmp_x[i] = tmp + h\n",
    "        fxh1 = f(tmp_x)\n",
    "        \n",
    "        # f(x - h)の計算\n",
    "        tmp_x[i] = tmp - h\n",
    "        fxh2 = f(tmp_x)\n",
    "        \n",
    "        grad[i] = (fxh1 - fxh2) / (2 * h)\n",
    "    \n",
    "    return grad\n",
    "\n",
    "\n",
    "# テスト\n",
    "print('numerical_gradient(f2, np.array([3, 4])) =', numerical_gradient(f2, np.array([3.0, 4.0])))\n",
    "print('numerical_gradient(f2, np.array([0, 2])) =', numerical_gradient(f2, np.array([0.0, 2.0])))\n",
    "print('numerical_gradient(f2, np.array([3, 0])) =', numerical_gradient(f2, np.array([3.0, 0.0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = [0 1 2 3 4 5 6 7 8 9]\n",
      "y = [0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# zeros_likeメソッドの動作確認\n",
    "x = np.arange(10)\n",
    "y = np.zeros_like(x)\n",
    "\n",
    "print('x =', x)\n",
    "print('y =', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.1 勾配法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 勾配法を実装する\n",
    "def gradient_desent(f, init_x, lr=0.01, step_num=100):\n",
    "    # init_xを変更させないようにするため\n",
    "    x = init_x.copy()\n",
    "    \n",
    "    for i in range(step_num):\n",
    "        grad = numerical_gradient(f, x)\n",
    "        x -= lr * grad\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -6.11110793e-10,   8.14814391e-10])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 実装した勾配法を試す\n",
    "gradient_desent(\n",
    "    f=f2,\n",
    "    init_x=np.array([-3.0, 4.0]),\n",
    "    lr=0.1,\n",
    "    step_num=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "学習率が大きすぎる例(lr = 10.0):  [ -2.58983747e+13  -1.29524862e+12]\n",
      "学習率が小さすぎる例(lr = 1e-10):  [-2.99999994  3.99999992]\n"
     ]
    }
   ],
   "source": [
    "init_x = np.array([-3.0, 4.0])\n",
    "\n",
    "# 学習率が大きすぎる例\n",
    "print('学習率が大きすぎる例(lr = 10.0): ', gradient_desent(f=f2, init_x=init_x, lr=10.0, step_num=100))\n",
    "print('学習率が小さすぎる例(lr = 1e-10): ', gradient_desent(f=f2, init_x=init_x, lr=1e-10, step_num=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.2 ニューラルネットワークに対する勾配"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "from common.functions import (softmax, cross_entropy_error)\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "class SimpleNet(object):\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2, 3)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return np.dot(x, self.W)\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        z = self.predict(x)\n",
    "        y = softmax(z)\n",
    "        loss = cross_entropy_error(y, t)\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net.W = [[-0.53572577  0.65692054  0.1165997 ]\n",
      " [-0.37309075 -1.30970997 -0.51098734]]\n",
      "p = [-0.65721714 -0.78458665 -0.38992879]\n",
      "np.argmax(p) = 2\n",
      "net.loss(x, t) = 0.891736898302\n"
     ]
    }
   ],
   "source": [
    "# SimpleNetを試す\n",
    "net = SimpleNet()\n",
    "print('net.W =', net.W)\n",
    "\n",
    "x = np.array([0.6, 0.9])\n",
    "p = net.predict(x)\n",
    "print('p =', p)\n",
    "print('np.argmax(p) =', np.argmax(p))\n",
    "\n",
    "t = np.array([0, 0, 1])\n",
    "print('net.loss(x, t) =', net.loss(x, t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dW = [[ 0.18827514  0.165759   -0.35403414]\n",
      " [ 0.28241271  0.24863849 -0.53105121]]\n"
     ]
    }
   ],
   "source": [
    "# なぜダミーでWを引数に設定するのか分からない\n",
    "def f(W):\n",
    "    return net.loss(x, t)\n",
    "\n",
    "dW = numerical_gradient(f, net.W)\n",
    "print('dW =', dW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5.1 2層ニューラルネットワークのクラス\n",
    "初めに、2層ニューラルネットワークを1つのクラスとして実装することから始める。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from common.functions import *\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "class TwoLayerNet(object):\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        self.params = {}\n",
    "\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        \n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        W1 = self.params['W1']\n",
    "        b1 = self.params['b1']\n",
    "        \n",
    "        W2 = self.params['W2']\n",
    "        b2 = self.params['b2']\n",
    "        \n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        \n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        \n",
    "        return cross_entropy_error(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        \n",
    "        return accuracy\n",
    "    \n",
    "    def numerical_gradient(self, x, t):\n",
    "        def loss_W(W):\n",
    "            return self.loss(x, t)\n",
    "            \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        \n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- 重みとバイアス -----\n",
      "net.params['W1'].shape = (784, 100)\n",
      "net.params['b1'].shape = (100,)\n",
      "net.params['W2'].shape = (100, 10)\n",
      "net.params['b2'].shape = (10,)\n"
     ]
    }
   ],
   "source": [
    "# TwoLayerNetを試す\n",
    "net = TwoLayerNet(input_size=784, hidden_size=100, output_size=10)\n",
    "\n",
    "print('----- 重みとバイアス -----')\n",
    "print(\"net.params['W1'].shape =\", net.params['W1'].shape)\n",
    "print(\"net.params['b1'].shape =\", net.params['b1'].shape)\n",
    "print(\"net.params['W2'].shape =\", net.params['W2'].shape)\n",
    "print(\"net.params['b2'].shape =\", net.params['b2'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- 推論処理 -----\n",
      "y = [[ 0.10096015  0.09518637  0.09740336  0.09756574  0.10103248  0.09787467\n",
      "   0.10726158  0.09685721  0.09747524  0.1083832 ]\n",
      " [ 0.10069684  0.09474417  0.09783472  0.0970222   0.1012451   0.0979781\n",
      "   0.10733542  0.09710959  0.09759657  0.10843729]\n",
      " [ 0.10097832  0.09507847  0.09771713  0.097256    0.1011199   0.09760068\n",
      "   0.1070632   0.09713756  0.09724943  0.10879932]\n",
      " [ 0.1011573   0.094865    0.09758215  0.09757701  0.10089678  0.09741444\n",
      "   0.10746093  0.09681855  0.09745138  0.10877645]\n",
      " [ 0.10076386  0.09504501  0.09739035  0.0975692   0.10135993  0.09774502\n",
      "   0.10691646  0.09686021  0.09756259  0.10878737]\n",
      " [ 0.10091879  0.0950426   0.09776218  0.09713894  0.1013519   0.09798323\n",
      "   0.1071294   0.09691193  0.09749831  0.10826271]\n",
      " [ 0.10102461  0.0950317   0.09786823  0.09735454  0.10109877  0.0977426\n",
      "   0.10702709  0.0969082   0.09778126  0.10816299]\n",
      " [ 0.10115677  0.09503313  0.09775602  0.09765026  0.10106137  0.09774827\n",
      "   0.1070723   0.09693788  0.09750531  0.10807869]\n",
      " [ 0.10123187  0.09504772  0.09765276  0.09732987  0.10105495  0.0976661\n",
      "   0.10691126  0.09705585  0.09750394  0.10854568]\n",
      " [ 0.10098642  0.09540599  0.09760002  0.09754696  0.1008363   0.09781056\n",
      "   0.10705377  0.09679237  0.09751849  0.10844913]\n",
      " [ 0.10106004  0.09532613  0.09738167  0.09739123  0.10168882  0.09761077\n",
      "   0.10726146  0.09661208  0.0972392   0.1084286 ]\n",
      " [ 0.10107473  0.09482079  0.09771406  0.09740328  0.10101107  0.09774929\n",
      "   0.10699819  0.09710437  0.09739014  0.10873409]\n",
      " [ 0.10106921  0.09506091  0.09758815  0.09720711  0.10134536  0.09740154\n",
      "   0.10712393  0.09691148  0.09771818  0.10857413]\n",
      " [ 0.10095882  0.09549482  0.0977485   0.09758296  0.10100121  0.09768744\n",
      "   0.10711082  0.09681552  0.0973619   0.10823802]\n",
      " [ 0.10092862  0.09491664  0.09796781  0.09728895  0.10126014  0.09782638\n",
      "   0.10726369  0.0972198   0.09730106  0.10802691]\n",
      " [ 0.10105963  0.09497769  0.09748648  0.09735901  0.10119128  0.09782105\n",
      "   0.10693806  0.09691238  0.09753218  0.10872223]\n",
      " [ 0.10085266  0.09525721  0.09790139  0.09762458  0.10078389  0.09796738\n",
      "   0.10699081  0.09705115  0.09722243  0.10834851]\n",
      " [ 0.10088184  0.09505209  0.09754095  0.09759622  0.10147979  0.0977419\n",
      "   0.10711278  0.09687958  0.09747354  0.10824131]\n",
      " [ 0.10121612  0.09516026  0.0976075   0.09725523  0.10130259  0.09765336\n",
      "   0.10730361  0.09683238  0.0973985   0.10827046]\n",
      " [ 0.10120321  0.09513559  0.09764652  0.09749384  0.10128236  0.09767837\n",
      "   0.10692961  0.09701409  0.09709561  0.10852081]\n",
      " [ 0.10092406  0.09534091  0.09763963  0.097298    0.10162639  0.09780995\n",
      "   0.10709001  0.09648294  0.09715532  0.10863278]\n",
      " [ 0.10078708  0.0949469   0.09784634  0.09755584  0.10114595  0.09767669\n",
      "   0.10719868  0.09701745  0.09743417  0.1083909 ]\n",
      " [ 0.10108433  0.09513909  0.09790036  0.09712682  0.10104594  0.09804273\n",
      "   0.10733578  0.09693657  0.09729093  0.10809745]\n",
      " [ 0.10078562  0.09499655  0.09753076  0.0973676   0.10132234  0.09807332\n",
      "   0.10673668  0.09685166  0.09791543  0.10842005]\n",
      " [ 0.10095993  0.09521166  0.09781038  0.09747734  0.1010953   0.09780108\n",
      "   0.10734876  0.09658194  0.09737295  0.10834065]\n",
      " [ 0.10106471  0.09515238  0.09762829  0.0977606   0.10104991  0.09760348\n",
      "   0.10728833  0.09690057  0.09744976  0.10810197]\n",
      " [ 0.10125064  0.09526018  0.09772001  0.09728383  0.10098571  0.09767902\n",
      "   0.10702646  0.09709364  0.09734825  0.10835228]\n",
      " [ 0.10099735  0.09502949  0.09768867  0.09702372  0.10136123  0.09795476\n",
      "   0.10715892  0.096872    0.09748982  0.10842403]\n",
      " [ 0.10115687  0.09480551  0.09788128  0.09727309  0.10136657  0.09762362\n",
      "   0.10720913  0.09718848  0.09726799  0.10822744]\n",
      " [ 0.10088426  0.09501155  0.09748392  0.09742345  0.10105872  0.09768047\n",
      "   0.10736568  0.0970808   0.09754001  0.10847115]\n",
      " [ 0.10138524  0.09507726  0.09769629  0.09744629  0.10079117  0.09795581\n",
      "   0.10711104  0.09689003  0.09724659  0.10840029]\n",
      " [ 0.10083594  0.09508822  0.09789114  0.09735454  0.10102113  0.09798065\n",
      "   0.10702067  0.09712064  0.09725741  0.10842966]\n",
      " [ 0.10120452  0.09510405  0.09749213  0.09750012  0.10105679  0.09785567\n",
      "   0.10674737  0.09677466  0.09751102  0.10875367]\n",
      " [ 0.10104322  0.09534748  0.09755509  0.09707075  0.10134261  0.09785255\n",
      "   0.10713871  0.09680044  0.09753797  0.10831117]\n",
      " [ 0.10104042  0.09511073  0.09745827  0.09757577  0.1008316   0.09808558\n",
      "   0.10688404  0.09691024  0.09756043  0.10854294]\n",
      " [ 0.10085689  0.09512595  0.0977175   0.09754758  0.10110817  0.09782184\n",
      "   0.10693483  0.09669796  0.09792161  0.10826767]\n",
      " [ 0.10095415  0.09501554  0.09751462  0.09748259  0.10122819  0.09763725\n",
      "   0.10701776  0.09705311  0.09761515  0.10848165]\n",
      " [ 0.10102876  0.09513824  0.09770288  0.09731319  0.1009418   0.09788077\n",
      "   0.10700743  0.09689526  0.09761637  0.10847531]\n",
      " [ 0.1013358   0.09513519  0.09768277  0.09729117  0.10113464  0.09772418\n",
      "   0.10706219  0.09679539  0.09727562  0.10856304]\n",
      " [ 0.10095815  0.0950478   0.09764711  0.09732984  0.10126047  0.09745331\n",
      "   0.10739885  0.09713243  0.09756046  0.10821158]\n",
      " [ 0.10101875  0.09528877  0.09769378  0.09717648  0.10088199  0.09790826\n",
      "   0.10721042  0.09708815  0.09754354  0.10818989]\n",
      " [ 0.10086981  0.09510771  0.09762539  0.09704821  0.10073816  0.09762057\n",
      "   0.10733194  0.09704003  0.09777271  0.10884546]\n",
      " [ 0.10110195  0.09501344  0.09760825  0.09719143  0.10119406  0.09788123\n",
      "   0.10686541  0.09714508  0.0974949   0.10850425]\n",
      " [ 0.10098379  0.09482957  0.09735486  0.09722634  0.10116107  0.09806066\n",
      "   0.10716476  0.09687524  0.09764816  0.10869556]\n",
      " [ 0.10085538  0.09499619  0.09762772  0.09709373  0.10111186  0.0978482\n",
      "   0.10761836  0.09709772  0.0974696   0.10828125]\n",
      " [ 0.1008443   0.09514052  0.09781407  0.09728205  0.1013972   0.09763632\n",
      "   0.10724796  0.09688012  0.09756084  0.10819663]\n",
      " [ 0.1008937   0.09526098  0.09810588  0.09753234  0.10113928  0.09772281\n",
      "   0.10705237  0.09667808  0.09762954  0.10798503]\n",
      " [ 0.10111171  0.09532178  0.09767277  0.09709467  0.10108035  0.09763503\n",
      "   0.10680626  0.09725146  0.09737732  0.10864865]\n",
      " [ 0.10081066  0.09497457  0.09764562  0.09788552  0.10095755  0.09786787\n",
      "   0.10712783  0.09673271  0.09756462  0.10843305]\n",
      " [ 0.1012234   0.0954116   0.0976653   0.09745516  0.100884    0.09768539\n",
      "   0.10714348  0.09695182  0.09731048  0.10826936]\n",
      " [ 0.1009361   0.09500926  0.097779    0.0976356   0.10118636  0.09755977\n",
      "   0.10700361  0.09719133  0.09740212  0.10829686]\n",
      " [ 0.10090978  0.09498223  0.09764174  0.09732331  0.10092158  0.09773888\n",
      "   0.10704905  0.09703919  0.09761179  0.10878245]\n",
      " [ 0.10100028  0.09487605  0.09754133  0.0971695   0.10129837  0.09772554\n",
      "   0.10752682  0.09720494  0.09742016  0.10823702]\n",
      " [ 0.10102451  0.09512212  0.09746056  0.09713998  0.10124551  0.09787059\n",
      "   0.10687738  0.09694291  0.09761293  0.10870351]\n",
      " [ 0.1007154   0.09525186  0.09784538  0.09723742  0.10061376  0.09784203\n",
      "   0.10721712  0.09696129  0.0977026   0.10861314]\n",
      " [ 0.10097575  0.09509521  0.09771357  0.09726814  0.10120802  0.09778782\n",
      "   0.10737957  0.0969844   0.0973488   0.10823873]\n",
      " [ 0.10098153  0.09548457  0.09716316  0.09748397  0.10113633  0.09781521\n",
      "   0.10693166  0.09675689  0.09751388  0.1087328 ]\n",
      " [ 0.10101354  0.09470038  0.09758038  0.09729873  0.10125563  0.09787281\n",
      "   0.10711018  0.09698273  0.09747862  0.10870701]\n",
      " [ 0.10119052  0.09519835  0.09740883  0.09710879  0.10115578  0.0976693\n",
      "   0.10714794  0.09660684  0.09767262  0.10884104]\n",
      " [ 0.10111543  0.09500796  0.09756709  0.09744629  0.10100118  0.09762625\n",
      "   0.10692792  0.0971152   0.0975299   0.10866279]\n",
      " [ 0.10117423  0.09525354  0.09795856  0.09749957  0.10111013  0.09757875\n",
      "   0.10712275  0.09697041  0.09715112  0.10818094]\n",
      " [ 0.10114411  0.09507359  0.09780356  0.0969158   0.10122315  0.09792348\n",
      "   0.10706122  0.09698198  0.09725191  0.10862122]\n",
      " [ 0.10077296  0.09517599  0.0975079   0.0973347   0.10132312  0.09791902\n",
      "   0.10721919  0.09687879  0.09746174  0.10840657]\n",
      " [ 0.10117044  0.09487828  0.09757112  0.09762366  0.10135775  0.09756659\n",
      "   0.10739258  0.09672584  0.09737421  0.10833953]\n",
      " [ 0.10115853  0.09542387  0.09763131  0.09726382  0.10099206  0.09755956\n",
      "   0.10726818  0.09705477  0.09724526  0.10840265]\n",
      " [ 0.10097355  0.09509481  0.09753652  0.09697522  0.10122649  0.09806358\n",
      "   0.10683047  0.09709823  0.0973178   0.10888334]\n",
      " [ 0.10110618  0.09545558  0.09772602  0.09749388  0.10092599  0.09774256\n",
      "   0.10714127  0.09690535  0.09744034  0.10806284]\n",
      " [ 0.10113893  0.09512451  0.09786304  0.0973021   0.10102394  0.09747553\n",
      "   0.10705216  0.09717484  0.09718295  0.108662  ]\n",
      " [ 0.10107606  0.09498228  0.09775557  0.09713384  0.10142585  0.09763102\n",
      "   0.10722679  0.09668864  0.09767675  0.1084032 ]\n",
      " [ 0.10089083  0.09520082  0.09789072  0.09746355  0.10104256  0.09776025\n",
      "   0.10700461  0.09684284  0.09763459  0.10826922]\n",
      " [ 0.10100864  0.09501533  0.0976695   0.0973684   0.10130738  0.09771143\n",
      "   0.1073515   0.09714165  0.09729102  0.10813515]\n",
      " [ 0.10109168  0.09510169  0.09757908  0.09742185  0.10113575  0.09790611\n",
      "   0.10692433  0.0967532   0.09773303  0.10835329]\n",
      " [ 0.10087991  0.09523231  0.09796954  0.09714368  0.10120996  0.09775898\n",
      "   0.10717764  0.09702666  0.0974534   0.10814792]\n",
      " [ 0.10123498  0.09527797  0.09734427  0.09703181  0.10118932  0.09810773\n",
      "   0.10715113  0.0967065   0.09749249  0.10846381]\n",
      " [ 0.10102904  0.09506244  0.09769609  0.09703397  0.10111188  0.09765978\n",
      "   0.10692036  0.09714397  0.09764819  0.10869429]\n",
      " [ 0.10083395  0.09506312  0.09757587  0.0974924   0.10111353  0.09775694\n",
      "   0.10714155  0.09710433  0.09727453  0.10864377]\n",
      " [ 0.10141524  0.09505299  0.09755845  0.09727344  0.10084205  0.09788066\n",
      "   0.10707967  0.09707322  0.09766372  0.10816056]\n",
      " [ 0.10098363  0.09497095  0.09766375  0.09706275  0.10122308  0.09796454\n",
      "   0.1068423   0.09706316  0.09774436  0.10848147]\n",
      " [ 0.100913    0.0949993   0.09751132  0.09694465  0.10132624  0.09794982\n",
      "   0.10731974  0.09710344  0.09755067  0.10838183]\n",
      " [ 0.10105181  0.09546068  0.09750554  0.09702256  0.10086348  0.09791125\n",
      "   0.10726421  0.09718761  0.09734234  0.1083905 ]\n",
      " [ 0.10101032  0.0954928   0.09782843  0.09717065  0.10089076  0.09765056\n",
      "   0.10696005  0.09691165  0.09751705  0.10856773]\n",
      " [ 0.10094747  0.09528346  0.09746043  0.09721567  0.10091199  0.09756162\n",
      "   0.10693477  0.09743425  0.09773273  0.10851761]\n",
      " [ 0.10089242  0.09531999  0.09752536  0.09761848  0.10121596  0.09784028\n",
      "   0.10689777  0.09660618  0.0976981   0.10838547]\n",
      " [ 0.10089428  0.09506389  0.09725718  0.09755157  0.1012089   0.09766285\n",
      "   0.10713285  0.09689135  0.09780697  0.10853015]\n",
      " [ 0.10087817  0.09541114  0.09745546  0.09722524  0.1010844   0.09783355\n",
      "   0.10697386  0.0969249   0.09747197  0.10874132]\n",
      " [ 0.10139407  0.09541902  0.09756466  0.09728853  0.10134534  0.09761935\n",
      "   0.10694882  0.09667137  0.09742416  0.10832468]\n",
      " [ 0.10123549  0.09471656  0.09772056  0.0974712   0.10159147  0.09778226\n",
      "   0.10691842  0.0965335   0.09755548  0.10847507]\n",
      " [ 0.10101602  0.09518247  0.09760181  0.09761483  0.10095786  0.09801622\n",
      "   0.10686738  0.09665453  0.09773593  0.10835296]\n",
      " [ 0.1009775   0.09511247  0.09780718  0.09751426  0.10164798  0.09793713\n",
      "   0.10691755  0.09676699  0.09718229  0.10813665]\n",
      " [ 0.10073955  0.09525184  0.09785287  0.09740126  0.10106929  0.09804726\n",
      "   0.10694419  0.09690754  0.09765968  0.10812651]\n",
      " [ 0.1009781   0.09552106  0.09768577  0.09700702  0.10131354  0.09778613\n",
      "   0.10707619  0.09702592  0.09736031  0.10824596]\n",
      " [ 0.1008804   0.09507729  0.09770263  0.09725819  0.1010408   0.09780586\n",
      "   0.10725062  0.09724987  0.09724996  0.10848438]\n",
      " [ 0.10094455  0.09522221  0.09766994  0.09717474  0.10119531  0.09759184\n",
      "   0.10652662  0.09710221  0.09794553  0.10862706]\n",
      " [ 0.10084313  0.09512232  0.09738359  0.09729707  0.10116255  0.09802726\n",
      "   0.10714556  0.09692534  0.09751375  0.10857943]\n",
      " [ 0.10075421  0.09510268  0.09779756  0.09724154  0.10105164  0.09782491\n",
      "   0.10729997  0.09702125  0.09755088  0.10835537]\n",
      " [ 0.10104529  0.09490387  0.09764902  0.09694994  0.1012988   0.09777697\n",
      "   0.10713546  0.09719554  0.09733968  0.10870543]\n",
      " [ 0.10100868  0.09528222  0.0977828   0.09722792  0.10115429  0.09777096\n",
      "   0.1069457   0.09725895  0.09731171  0.10825676]\n",
      " [ 0.10084063  0.09552156  0.09773184  0.0969552   0.10093324  0.09751239\n",
      "   0.10714301  0.09714503  0.09754089  0.10867621]\n",
      " [ 0.10102346  0.09514744  0.09751958  0.09736802  0.10091733  0.09753782\n",
      "   0.1072093   0.09718266  0.09765134  0.10844305]\n",
      " [ 0.10077017  0.09516726  0.09793005  0.09753116  0.10107706  0.09766492\n",
      "   0.1069286   0.09705636  0.09740477  0.10846964]]\n"
     ]
    }
   ],
   "source": [
    "# 推論処理\n",
    "x = np.random.rand(100, 784)\n",
    "y = net.predict(x)\n",
    "\n",
    "print('----- 推論処理 -----')\n",
    "print('y =', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- 勾配の計算 -----\n",
      "grads['W1'] = (784, 100)\n",
      "grads['b1'] = (100,)\n",
      "grads['W2'] = (100, 10)\n",
      "grads['b2'] = (10,)\n"
     ]
    }
   ],
   "source": [
    "# 勾配の計算\n",
    "x = np.random.rand(100, 784)\n",
    "t = np.random.rand(100, 10)\n",
    "\n",
    "grads = net.numerical_gradient(x, t)\n",
    "\n",
    "print('----- 勾配の計算 -----')\n",
    "print(\"grads['W1'] =\", grads['W1'].shape)\n",
    "print(\"grads['b1'] =\", grads['b1'].shape)\n",
    "print(\"grads['W2'] =\", grads['W2'].shape)\n",
    "print(\"grads['b2'] =\", grads['b2'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5.2 ミニバッチ学習の実装\n",
    "TwoLayerNetクラスを対象に、MNISTデータセットを使って学習を行う。\n",
    "\n",
    "## 6時間近く回しても結果が返って来なかった、、、"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-ff48441dde2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# 勾配の計算\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumerical_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m# パラメータの更新\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-13295dcf3d29>\u001b[0m in \u001b[0;36mnumerical_gradient\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W1'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumerical_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_W\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'b1'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumerical_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_W\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'b1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ishiyama/deep_learning/common/gradient.py\u001b[0m in \u001b[0;36mnumerical_gradient\u001b[0;34m(f, x)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_val\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mfxh2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# f(x-h)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0mgrad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfxh1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mfxh2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-13295dcf3d29>\u001b[0m in \u001b[0;36mloss_W\u001b[0;34m(W)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnumerical_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mloss_W\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-13295dcf3d29>\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcross_entropy_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-13295dcf3d29>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mz1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0ma2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "# 下記のモジュールは上で実装したTwoLayerNetと同じため、ここではコメントアウトしておく\n",
    "# sys.path.append('/home/ishiyama/deep_learning/deep-learning-from-scratch/ch04')\n",
    "# from two_layer_net import TwoLayerNet\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "train_loss_list = []\n",
    "\n",
    "# hyperparameters\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    # ミニバッチの取得\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 勾配の計算\n",
    "    grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    \n",
    "    # パラメータの更新\n",
    "    for key in network.params:\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    # 学習経過の記録\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 4.5.3 テストデータで評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'numpy.ndarray' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-c3b275e5f3a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miters_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mbatch_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mx_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mt_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'numpy.ndarray' object is not callable"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "# from two_layer_net import TwoLayerNet\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "# 1エポックあたりの繰り返し数\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "# ハイパーパラメータ\n",
    "iters_num = 10000\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train(batch_mask)\n",
    "    t_batch = t_train(batch_mask)\n",
    "    \n",
    "    # 勾配の計算\n",
    "    grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    \n",
    "    # パラメータの更新\n",
    "    for key in grad:\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    # 1エポックごとに認識精度を計算\n",
    "    if not (i % iter_per_epoch):\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print('train_acc, test_acc | {}, {}'.format(train_acc, test_acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考文献\n",
    "斎藤(2016), ゼロから作るDeep Learning, O'reilly Japan  \n",
    "Bishop(2006), Pattern Recognition and Machine Learning, Springer-Verlag New York"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
